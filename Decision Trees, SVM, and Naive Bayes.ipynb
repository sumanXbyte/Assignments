{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2560f1",
   "metadata": {},
   "source": [
    "1: What is Information Gain, and how is it used in Decision Trees?\n",
    "-> Information Gain is a metric used in Decision Trees to measure how much uncertainty (entropy) is reduced after splitting a dataset based on a particular feature. The feature with the highest Information Gain is selected for splitting the node.\n",
    "\n",
    "2: What is the difference between Gini Impurity and Entropy?\n",
    "-> Gini Impurity and Entropy are both impurity measures used to evaluate splits in Decision Trees, but they differ in calculation and sensitivity.\n",
    "Gini Impurity is faster then Entropy, Gini Impurity is less sensitive then Entropy, Gini Impurity use CART trees and Entropy use ID3, C4.5\n",
    "\n",
    "\n",
    "3: What is Pre-Pruning in Decision Trees?\n",
    "-> Pre-pruning is a technique used to stop the growth of a Decision Tree early to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb3d1ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances: [0.01333333 0.         0.56405596 0.42261071]\n"
     ]
    }
   ],
   "source": [
    "# 4: Python program to train a Decision Tree using Gini Impurity\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Train Decision Tree with Gini\n",
    "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Feature Importances:\", model.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f509bc52",
   "metadata": {},
   "source": [
    "5: What is a Support Vector Machine (SVM)?\n",
    "-> SVM is a supervised learning algorithm that finds the optimal hyperplane which maximizes the margin between different classes.\n",
    "\n",
    "6: What is the Kernel Trick in SVM?\n",
    "-> The Kernel Trick allows SVM to operate in higher-dimensional space without explicitly computing the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837b9c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel Accuracy: 0.9814814814814815\n",
      "RBF Kernel Accuracy: 0.7592592592592593\n"
     ]
    }
   ],
   "source": [
    "# 7: Python program to compare Linear and RBF SVM\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_wine(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Linear SVM\n",
    "linear_svm = SVC(kernel='linear')\n",
    "linear_svm.fit(X_train, y_train)\n",
    "linear_acc = accuracy_score(y_test, linear_svm.predict(X_test))\n",
    "\n",
    "# RBF SVM\n",
    "rbf_svm = SVC(kernel='rbf')\n",
    "rbf_svm.fit(X_train, y_train)\n",
    "rbf_acc = accuracy_score(y_test, rbf_svm.predict(X_test))\n",
    "\n",
    "print(\"Linear Kernel Accuracy:\", linear_acc)\n",
    "print(\"RBF Kernel Accuracy:\", rbf_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc70137",
   "metadata": {},
   "source": [
    "8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
    "-> Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem and assumes independence between features.\n",
    "\n",
    "9: Differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes\n",
    "-> | Type           | Data Type   | Use Case             |\n",
    "| -------------- | ----------- | -------------------- |\n",
    "| Gaussian NB    | Continuous  | Medical, sensor data |\n",
    "| Multinomial NB | Count-based | Text, NLP            |\n",
    "| Bernoulli NB   | Binary      | Spam detection       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af35cbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9415204678362573\n"
     ]
    }
   ],
   "source": [
    "# 10: Gaussian Naïve Bayes on Breast Cancer Dataset\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
