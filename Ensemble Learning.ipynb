{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53851f1",
   "metadata": {},
   "source": [
    "1: What is Ensemble Learning in Machine Learning? Explain the key idea behind it.\n",
    "-> Ensemble Learning is a machine learning technique where multiple models (called base learners) are trained and combined to solve the same problem, with the goal of achieving better performance, higher accuracy, and improved robustness compared to a single model.\n",
    "\n",
    "Key Idea:\n",
    "\n",
    "Instead of relying on one model, ensemble learning aggregates predictions from several models so that individual weaknesses are compensated by others.\n",
    "\n",
    "2: What is the difference between Bagging and Boosting?\n",
    "-> | Aspect          | Bagging                          | Boosting                                |\n",
    "| --------------- | -------------------------------- | --------------------------------------- |\n",
    "| Full form       | Bootstrap Aggregating            | Sequential Error Correction             |\n",
    "| Training        | Parallel                         | Sequential                              |\n",
    "| Data sampling   | Random sampling with replacement | Weighted sampling                       |\n",
    "| Focus           | Reduce variance                  | Reduce bias                             |\n",
    "| Handling errors | All models treated equally       | Misclassified samples get higher weight |\n",
    "| Example         | Random Forest                    | AdaBoost, Gradient Boosting             |\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Bagging is best when models overfit.\n",
    "\n",
    "Boosting is best when models underfit.\n",
    "\n",
    "3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
    "-> Bootstrap sampling is a technique where multiple datasets are created by randomly sampling the original dataset with replacement.\n",
    "\n",
    "Role in Bagging:\n",
    "\n",
    "Each model is trained on a different bootstrap sample\n",
    "\n",
    "Some data points may appear multiple times\n",
    "\n",
    "Some may not appear at all\n",
    "\n",
    "In Random Forest:\n",
    "\n",
    "Each decision tree gets a different bootstrap sample\n",
    "\n",
    "Leads to diverse trees\n",
    "\n",
    "Reduces correlation among trees\n",
    "\n",
    "End Result:\n",
    "\n",
    "Improved stability and reduced variance of the final model.\n",
    "\n",
    "\n",
    ": What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
    "-> Out-of-Bag (OOB) samples are the data points not selected during bootstrap sampling for a particular model.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "- Around 36.8% of data becomes OOB for each tree\n",
    "\n",
    "- These samples act as validation data\n",
    "\n",
    "OOB Score Usage:\n",
    "\n",
    "- Predictions are made on OOB samples\n",
    "\n",
    "- Accuracy is calculated without a separate test set\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- No need for cross-validation\n",
    "\n",
    "- Saves computation\n",
    "\n",
    "- Provides unbiased performance estimate\n",
    "\n",
    "5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
    "-> | Aspect          | Decision Tree      | Random Forest       |\n",
    "| --------------- | ------------------ | ------------------- |\n",
    "| Stability       | High variance      | Stable              |\n",
    "| Sensitivity     | Sensitive to noise | Robust              |\n",
    "| Importance bias | Can overfit        | Averaged importance |\n",
    "| Reliability     | Low                | High                |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ff52cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst area              0.139357\n",
      "worst concave points    0.132225\n",
      "mean concave points     0.107046\n",
      "worst radius            0.082848\n",
      "worst perimeter         0.080850\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 6: Random Forest on Breast Cancer Dataset (Feature Importance)\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Feature importance\n",
    "importance = pd.Series(rf.feature_importances_, index=data.feature_names)\n",
    "top5 = importance.sort_values(ascending=False).head(5)\n",
    "\n",
    "print(top5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a001a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n",
      "Bagging Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 7: Bagging Classifier vs Single Decision Tree (Iris Dataset)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Single Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
    "\n",
    "# Bagging Classifier\n",
    "bag = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "bag.fit(X_train, y_train)\n",
    "bag_acc = accuracy_score(y_test, bag.predict(X_test))\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", dt_acc)\n",
    "print(\"Bagging Accuracy:\", bag_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac51e277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 5, 'n_estimators': 100}\n",
      "Best Accuracy: 0.9596180717279925\n"
     ]
    }
   ],
   "source": [
    "# 8: Random Forest with GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid = GridSearchCV(rf, param_grid, cv=5)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Accuracy:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a0ac81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging MSE: 0.2582487035129332\n",
      "Random Forest MSE: 0.25424371393528344\n"
     ]
    }
   ],
   "source": [
    "# 9: Bagging Regressor vs Random Forest Regressor\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "bag = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Bagging MSE:\", mean_squared_error(y_test, bag.predict(X_test)))\n",
    "print(\"Random Forest MSE:\", mean_squared_error(y_test, rf.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112752a",
   "metadata": {},
   "source": [
    "10: Ensemble Learning for Loan Default Prediction\n",
    "-> Step-by-Step Approach:\n",
    "1. Choosing Bagging or Boosting\n",
    "\n",
    "If data is noisy → Bagging\n",
    "\n",
    "If patterns are complex → Boosting\n",
    "\n",
    "For loan default → Boosting preferred (better bias handling)\n",
    "\n",
    "2. Handling Overfitting\n",
    "\n",
    "Cross-validation\n",
    "\n",
    "Early stopping\n",
    "\n",
    "Regularization\n",
    "\n",
    "Limiting tree depth\n",
    "\n",
    "3. Selecting Base Models\n",
    "\n",
    "Decision Trees (interpretable)\n",
    "\n",
    "Logistic Regression (baseline)\n",
    "\n",
    "Gradient Boosted Trees (performance)\n",
    "\n",
    "4. Performance Evaluation\n",
    "\n",
    "K-Fold Cross-Validation\n",
    "\n",
    "ROC-AUC\n",
    "\n",
    "Precision-Recall\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "5. Why Ensemble Improves Decisions\n",
    "\n",
    "Reduces financial risk\n",
    "\n",
    "More stable predictions\n",
    "\n",
    "Handles class imbalance\n",
    "\n",
    "Improves regulatory trust\n",
    "\n",
    "Final Impact:\n",
    "\n",
    "Ensemble learning leads to accurate, robust, and fair loan approval decisions, minimizing default risk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
