{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a8b0fc",
   "metadata": {},
   "source": [
    "1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
    "-> K-Nearest Neighbors (KNN) is a supervised, instance-based learning algorithm that makes predictions by finding the K closest data points to a new input based on a distance metric.\n",
    "\n",
    "Classification: Uses majority voting among neighbors\n",
    "\n",
    "Regression: Uses average (or weighted average) of neighbors’ values\n",
    "\n",
    "2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
    "-> The Curse of Dimensionality refers to problems that arise when number of features increases, causing distance-based models like KNN to lose effectiveness.\n",
    "\n",
    "3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
    "-> PCA is an unsupervised dimensionality reduction technique that transforms features into new orthogonal components capturing maximum variance.\n",
    "\n",
    "4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
    "-> Eigenvectors → directions of maximum variance\n",
    "\n",
    "Eigenvalues → magnitude of variance along those directions\n",
    "\n",
    "\n",
    "5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
    "-> PCA reduces dimensionality → KNN performs better and faster.\n",
    "\n",
    "Problems with KNN\n",
    "\n",
    "Sensitive to high dimensions\n",
    "Computationally heavy\n",
    "\n",
    "PCA helps by\n",
    "\n",
    "Reducing noise\n",
    "Making distance meaningful\n",
    "Lowering computation\n",
    "\n",
    "End Result\n",
    "\n",
    "Higher accuracy\n",
    "Better generalization\n",
    "Faster predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b7f8a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7407407407407407, 0.9629629629629629)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy.\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Without scaling\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "acc_without_scaling = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# With scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = knn.predict(X_test_scaled)\n",
    "acc_with_scaling = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "acc_without_scaling, acc_with_scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf866e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36198848, 0.1920749 , 0.11123631, 0.0706903 , 0.06563294,\n",
       "       0.04935823, 0.04238679, 0.02680749, 0.02222153, 0.01930019,\n",
       "       0.01736836, 0.01298233, 0.00795215])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7: Train a PCA model and print explained variance ratio.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5172de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9814814814814815"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8: Train KNN on PCA-transformed data (top 2 components).\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pca, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8941578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclidean 0.9629629629629629\n",
      "manhattan 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "# 9: Compare KNN with different distance metrics.\n",
    "\n",
    "for metric in ['euclidean', 'manhattan']:\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "    print(metric, accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c2c99",
   "metadata": {},
   "source": [
    "10: High-Dimensional Gene Expression Dataset – PCA + KNN Pipeline\n",
    "-> (Conceptual + Justification)\n",
    "\n",
    "Step 1: PCA\n",
    "\n",
    "Remove noise\n",
    "Handle multicollinearity\n",
    "Reduce overfitting\n",
    "\n",
    "Step 2: Choose components\n",
    "Scree plot\n",
    "Cumulative variance ≥ 95%\n",
    "\n",
    "Step 3: Apply KNN\n",
    "After PCA, distances become meaningful\n",
    "\n",
    "Step 4: Evaluation\n",
    "\n",
    "Cross-validation\n",
    "Accuracy, F1-score\n",
    "Confusion matrix\n",
    "\n",
    "Step 5: Stakeholder Justification\n",
    "Robust against overfitting\n",
    "Interpretable pipeline\n",
    "Industry-accepted approach\n",
    "Computationally efficient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
